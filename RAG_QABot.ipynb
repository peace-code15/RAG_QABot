{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zrdr1RDcpJ8N"
      },
      "outputs": [],
      "source": [
        "# using OpenAI API for generation and Pinecone as the vector database for retrieval.\n",
        "\n",
        "# Step 1: Install Required Libraries\n",
        "!pip install openai pinecone-client\n",
        "\n",
        "# Step 2: Import Required Modules\n",
        "import openai\n",
        "import pinecone\n",
        "\n",
        "# Step 3: Set Up API Keys\n",
        "openai.api_key = \"your_openai_api_key\"\n",
        "pinecone_api_key=\"your_api_key\"\n",
        "\n",
        "# Step 2: Initialize Pinecone\n",
        "pc = Pinecone(api_key=pinecone_api_key)\n",
        "\n",
        "\n",
        "# Step 4: Create a Pinecone Index\n",
        "# Pinecone indexes are used to store vector embeddings of documents for efficient retrieval.\n",
        "index_name = \"qa-index\"\n",
        "\n",
        "if index_name not in pc.list_indexes().names():\n",
        "    pc.create_index(\n",
        "        name=index_name,\n",
        "        dimension=1536,\n",
        "        metric=\"cosine\",\n",
        "        spec=ServerlessSpec(\n",
        "            cloud=\"aws\",\n",
        "            region=\"us-east-1\"\n",
        "        )\n",
        "      )\n",
        "\n",
        "# 1536 is the embedding size for OpenAI embeddings\n",
        "\n",
        "index = pc.Index(index_name)\n",
        "print(f\"Connected to index: {index_name}\")\n",
        "\n",
        "# Step 5: Defining a Function to Add Documents to the Index\n",
        "# This function converts text into embeddings and adds them to the Pinecone index.\n",
        "\n",
        "def add_to_index(documents):\n",
        "\n",
        "    embeddings = openai.Embedding.create(\n",
        "        input=documents, model=\"text-embedding-ada-002\"\n",
        "    )\n",
        "    for doc, embed in zip(documents, embeddings['data']):\n",
        "        index.upsert([(embed['index'], embed['embedding'], {\"text\": doc})])\n",
        "\n",
        "# Example documents\n",
        "documents = [\n",
        "    \"Our business offers 24/7 customer support.\",\n",
        "    \"The refund policy allows returns within 30 days of purchase.\",\n",
        "    \"We provide free shipping on orders over $50.\"\n",
        "]\n",
        "add_to_index(documents)\n",
        "\n",
        "# Step 6: Define the RAG Retrieval Function\n",
        "# This function retrieves relevant documents based on a query and generates a response.\n",
        "\n",
        "def retrieve_and_generate(query):\n",
        "    \"\"\"Retrieve relevant documents and generate a response using OpenAI.\"\"\"\n",
        "\n",
        "    query_embedding = openai.Embedding.create(\n",
        "        input=[query], model=\"text-embedding-ada-002\"\n",
        "    )['data'][0]['embedding']\n",
        "    results = index.query(query_embedding, top_k=3, include_metadata=True)\n",
        "\n",
        "    # Extract texts from results\n",
        "    context = \"\\n\".join([match['metadata']['text'] for match in results['matches']])\n",
        "\n",
        "    # It will generate response using OpenAI\n",
        "    response = openai.Completion.create(\n",
        "        engine=\"text-davinci-003\",\n",
        "        prompt=f\"Answer the question based on the context:\\nContext: {context}\\nQuestion: {query}\\nAnswer:\",\n",
        "        max_tokens=100\n",
        "    )\n",
        "    return response['choices'][0]['text'].strip()\n",
        "\n",
        "# Example query\n",
        "query = \"What is the refund policy?\"\n",
        "response = retrieve_and_generate(query)\n",
        "print(\"Response:\", response)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JmsmOLH9pK8S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MJrbXxTOpK_n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XQOdPZv-pLCy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}